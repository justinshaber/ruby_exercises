PROBLEM:
  OVERVIEW: Write a program that, given a natural number and a set of one or more other numbers, can find the sum of all the multiples of the numbers in the set that are less than the first number. If the set of numbers is not given, use a default set of 3 and 5.

  INPUT: integers
    - max number
    - optional - set of numbers, default is 3 & 5

  OUTPUT: integer - sum of the multiples of set numbers up to, but not including, max number.

EXAMPLES/RULES
  For instance, if we list all the natural numbers up to, but not including, 20 that are multiples of either 3 or 5, we get 3, 5, 6, 9, 10, 12, 15, and 18. The sum of these multiples is 78.
  - no duplicate multiples

  METHODS
    #initialize
      - determines set numbers
        - default is 3,5
        - or explicitly passed 
    SumOfMultiples#to
      - performs main logic
      - returns desired integer - sum of multiples
    #to is a class method and an instance method

DATA STRUCTURES
  Arrays
  * operator?

ALGORITHM
  #initialize
    INITIALIZES
      - accepts one arg
      - instance var set_vals
    CLASS METHOD #to
      - instantiates new SumOfMultiples object passing two integer arguments (3,5)
      - calls SumOfMultiples#to
    SumOfMultiples#to
      - accepts one argument - target_num
      - INITIALIZE multiples_arr
      - ITERATE through each set number
        SET COUNTER to 1
        LOOP until counter * current_set_num >= target_num
          PUSH counter * current_set_num into multiples_arr
      - REMOVE duplicate multiples
      - RETURN SUM of multiples_arr
        


